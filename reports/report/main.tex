% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage[usenames,dvipsnames]{xcolor}
% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CLAfICLe: Cross Lingual Adaptation for In-Context Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Giulio Starace \\
  University of Amsterdam / Amsterdam, The Netherlands \\
  \texttt{giulio.starace@gmail.com} \\}

% user settings
\usepackage{graphicx} % for images
\usepackage{subcaption} % for subfigures
\graphicspath{{../figures/}}
\usepackage[export]{adjustbox}
\usepackage{geometry}
\usepackage{booktabs}


\begin{document}
\maketitle
\begin{abstract}

	As the field of NLP becomes enveloped with pre-trained large language models (LLMs), it becomes
	more and more dependent on data and compute. In parallel, fine-tuning paradigms such as in-context
	learning (ICL) have emerged to address these requirements. Unfortunately, most of this research
	has only been conducted in English and is either prohibitively expensive or impossible to repeat
	in other languages. Multilingual LLMs have been proposed to address this issue, but have been
	shown to be outperformed by their monolingual counterparts. While research on the language
	adaptation of monolingual models shows promising results, this typically focuses on encoder-only
	transformers and in the decoder-only setting limit themselves to intrinsic evaluation. In this
	work, we tackle the problem of the cross-lingual adaptation of monolingual models fine-tuned to
	perform ICL. We combine state of the art language and task adaptation techniques and show that it
	is still difficult to outperform a simple baseline consisting of sandwhiching the model between
	translation API calls. Finally, we introduce a novel technique for post-hoc disentanglement,
	PHoDiVA, and propose directions for future research.

\end{abstract}
\section{Introduction}

Pre-trained large language models (LLMs) are dominating natural language processing (NLP) research
for tackling downstream tasks \citep{devlin_bert_2019,raffel_exploring_2020,brown_language_2020}.
These models rely on the availability of vast amounts of unsupervised training data and the high
usage computing resources that can be leveraged by variants of the transformer architecture
\citep{vaswani_attention_2017}. Because access to such data and compute is limited, and due to the
anglocentric nature of the field, the majority LLM research and application prioritises the English
language. This leads to a large gap between what can be achieved in English and other languages.
Research in multilingual LLMs attempts to address this issue, with encouraging results in many
aspects \citep{conneau_unsupervised_2020,bigscience_workshop_bloom_2022}. These multilingual models
however have been shown to underperform against monolingual counterparts \citep{wu_are_2020}, and
datasets for more niche applications such as fine-tuning for zero-shot and in-context prompted
generalisation remain almost exclusively in English
\citep{bach_promptsource_2022,mishra_cross-task_2022}. One approach to address this issue is
developing techniques for adapting existing English models to work in other languages. Recent
research in model adaptation has shown promising results
\citep{houlsby_parameter-efficient_2019,ainsworth_git_2022}, and there already exist some works
applying these techniques directly to the problem of cross-lingual transfer
\citep{artetxe_cross-lingual_2020}. These works however mainly focus either on encoder-only
transformer (EOT) variants or on performance on a few downstream tasks, through the use of
additional language- and/or task-specific fine-tuning
\citep{de_vries_adapting_2021,gogoulou_cross-lingual_2022}. Works that consider decoder-only
transformer (DOT) variants on the other hand \citep{de_vries_as_2021, minixhofer_wechsel_2022} limit
the scope to pretrained variants and intrinsic evaluation, with little focus on how their techniques
interact with \textit{fine-tuned} models and their performance on downstream tasks.

This work instead considers techniques for the efficient cross-lingual transfer of models fine-tuned
on \textit{in-context learning} (ICL). Here, the models are fine-tuned to leverage information
presented in the context window to address some downstream task, demonstrating improved performance
and generalisation \citep{wei_finetuned_2021,sanh_multitask_2022,wang_benchmarking_2022}, enabling
multi-task learning and eliminating the need for task-specific fine-tuning. Scaled versions of these
models \citep{chung_scaling_2022} are now on par with the best models from the similarly emerging
paradigm of LLM training with reinforcement learning from human feedback (RLHF)
\citep{ouyang_training_2022}. Lamentably, just like their training, the evaluation of these ICL
fine-tuned models relies on instruction and prompt templates, which are mainly available only in
English. This renders any cross-lingual adaptation of these models futile, as there is no way to
extrinsically evaluate them in the target language. Recent work from \citet{min_metaicl_2022}
circumvents this requirement by directly fine-tuning a model on chains of input-output pairs from
a suite of tasks, matching the performance of instruction-based ICL. We therefore focus on adapting
models trained under this particular framework, and contribute the following:
\begin{enumerate}
	\item We show that \citet{minixhofer_wechsel_2022}'s WECHSEL language-adaptation technique scales,
	      successfully adapting the large variant of GPT2 (774M) to French and German. We release the our
	      checkpoints, which previously did not exist at this parameter scale for these languages.
	\item We continue the evaluation of WECHSEL by assessing its robustness, applying it to
	      a fine-tuned variant of GPT2 and measuring its performance on a number of downstream tasks,
	      rather than just examining perplexity.
	\item We share our methods and results for adapting fine-tuned models capable of ICL from English to
	      French and German.
	\item We introduce the notion of ``targeted distillation'', a form of post-hoc disentanglement
	      leveraging adapters \citep{houlsby_parameter-efficient_2019} to extract only the fine-tuned
	      information from a fine-tuned model.  We refer to our technique as \textsc{PHoDiVA}
	      (\textbf{P}ost \textbf{Ho}c \textbf{Di}sentanglement via \textbf{V}essel \textbf{A}dapters).
\end{enumerate}
Surprisingly, we fail to match the performance a simple baseline consisting of sandwhiching the
model between translation API calls, which performs almost on par with the original model. We
hypothesize this may be due to under-trained models, and propose directions for future research.

\section{Related Work}

\subsection{In-context learning}

ICL in NLP is a paradigm most famously popularized by \citet{brown_language_2020}, where the input
in a transformer's context window is augmented with some additional information useful for some
downstream task. The model is said to ``learn''  from (\textit{sc.} leverage) this information,
without the need for any parameter updates. A very basic example of ICL is ``prompting'' the model
by prefixing the input with task-specific instructions. ICL provides a number of advantages. Because
the input is entirely in natural language, it acts an interpretable interface for human-model
interaction. As mentioned, ICL does not require any parameter updates, greatly reducing the
computational costs necessary and enabling the multi-task generalisation necessary for
language-model-as-a-service applications \citep{sun_black-box_2022}. Considerable research attention
has been devoted to the paradigm \citep{liu_what_2022,lu_fantastically_2022,wu_self-adaptive_2022},
with many contributions focusing on informal reasoning performance through scratchpads
\citep{nye_show_2021}, bootstrapping \citep{zelikman_star_2022}, chain-of-thought prompting
\citep{wei_chain--thought_2022,wang_self-consistency_2023}, learned verifiers
\citep{cobbe_training_2021} and selection-inference \citep{creswell_selection-inference_2023}
techniques among others. \citet{dohan_language_2022} propose a framework for formalizing these
techniques while \citet{zhao_calibrate_2021} propose solutions to the performance sensitivity to
prompt choice and ordering. Unsurprisingly, some studies find a performance boost can be achieved by
fine-tuning the models directly on ICL examples \citep{sanh_multitask_2022,wang_benchmarking_2022}
and develop template-based datasets for this purpose
\citep{bach_promptsource_2022,mishra_cross-task_2022}. To circumvent the variability or need for
templates, \citet{lester_power_2021} fine-tune a ``soft'' prompt instead of the task, while
\citet{min_metaicl_2022} directly use chains of input-output pairs for fine-tuning rather than
templates. All of these approaches limit themselves to English, leaving a large gap in multilingual
NLP. Our work attempts to partly address this gap by exploring ways of adapting ICL-fine-tuned
English models to other languages.

\subsection{Multilingual NLP}

Democratizing LLMs to other languages has mainly been achieved by repeating the pre-training process
on massive multi-lingual corpora. While non-transformer approaches exist
\citep{artetxe_massively_2019}, most attention is devoted to transformers, resulting in
encoder-only, encoder-decoder and decoder-only models such as XLM-R
\citep{conneau_cross-lingual_2019}, mT5 \citep{xue_mt5_2021}, XLGM \citep{lin_few-shot_2021}, mBART
\citep{liu_multilingual_2020} and BLOOM \citep{bigscience_workshop_bloom_2022}. While undoubtedly
valuable, these models tend to suffer from the \textit{curse of multilinguality}
\citep{conneau_unsupervised_2020}, where the performance degrades as the number of languages
increases. \citet{nozza_what_2020} and \citet{wu_are_2020} reproduce this claim, showing that
monolingual models tend to outperform multilingual models on downstream tasks. To address this
issue, researchers have re-trained transformers in other languages
\citep{martin_camembert_2020,de_vries_bertje_2019,chan_germans_2020, de_mattei_geppetto_2020} but
this approach may not scale well to all 200+ languages of the world. Instead, recent work has
focused on efficiently transferring monolingual English representation to other target languages.
\citet{artetxe_cross-lingual_2020} first tackle this problem with the aim of testing
\citet{pires_how_2019} and \citet{cao_multilingual_2022}'s hypothesis that joint training across
multiple languages on a shared vocabulary gives rise to cross-lingual representations that
generalise across languages. They propose a new method for monolingual representation transfer
consisting in retraining the lexical embeddings in the target language while keeping the rest of the
model frozen. \citet{de_vries_adapting_2021} successfully apply this method to low-resource target
languages and \citet{de_vries_as_2021} extend the method to DOT, successfully adapting GPT2 to
Italian and Dutch. Recently, \citet{minixhofer_wechsel_2022} has optimized the method by using
dictionaries of parallel static word embeddings to reinitialize the embeddings in an efficient way
before re-training them on the target language. Unfortunately in the DOT setting, none of these
methods consider variants larger than GPT2-small (117M parameters) and crucially focus only on
intrinsic evaluation rather than performance on downstream tasks. Furthermore, it remains to be seen
whether any of these methods can be applied to fine-tuned versions of GPT2, without leading to
catastrophic forgetting. Our work attempts to address these issues by exploring the transferability
MetaICL \citep{min_metaicl_2022} from English to French and German.

\subsection{Adaptation}

Adapters \citep{houlsby_parameter-efficient_2019} are an emerging method in NLP for the efficient
fine-tuning of LLMs, operating by injecting a small number of trainable parameters into the layers
of a pre-trained base. Originating from computer vision \citep{rebuffi_learning_2017}, in NLP they
have mainly been used for adapting to new tasks \citep{stickland_bert_2019} and domains
\citep{bapna_simple_2019} while avoiding catastrophic forgetting
\citep{mccloskey_catastrophic_1989}. There exist a few methods employing adapters for adapting to
new languages \citep{pfeiffer_mad-x_2020, ustun_hyper-x_2022}, but these methods generally rely on
multilingual transformer bases. In our work, we explore the use of adapters for separating
fine-tuning from language-adaptation, so to avoid catastrophic forgetting occurring during the
latter. In parallel, \citet{marchisio_mini-model_2022} propose a similar method which trains a small
model in parallel to the base and then operates over the smaller model for adaptation. Other
non-adapter approaches include meta-learning, which has also been applied to language adaptation
\citep{nooralahzadeh_zero-shot_2020}.

\subsection{Other related work}

Our work makes brief references to the area of distillation \citep{hinton_distilling_2015}, which is
typically defined to be the process of transferring information from a large model to a smaller one.
We also briefly touch on disentanglement \citep{bengio_representation_2013}, wherein representations
are ``disentangled'' such that a change in one dimension corresponds to a change in one factor of
variation but does not affect other factors. In our case, we are interested in a ``post-hoc''
disentanglement of the representations responsible for the fine-tuned capabilities of a base model,
similarly to \citet{khrulkov_disentangled_2021}.

\section{Method}\label{sec:method}

\begin{figure*}[ht]
	\subcaptionbox{\label{fig:sandwich}}
	{\includegraphics{sandwich.pdf}}
	\hfill
	\subcaptionbox{\label{fig:metaicl-gewechselt}}
	{\includegraphics{metaicl-gewechselt.pdf}}
	\hfill
	\subcaptionbox{}
	{\includegraphics{gpt2-gewechselt+metaicla.pdf}}
	\hfill
	\subcaptionbox{}
	{\includegraphics{gpt2-gewechselt+metaiclva.pdf}}
	\caption{Overview of each of the models evaluated in one of the two TGT
		languages (French or German). The baseline
		\textcolor[HTML]{79d6ae}{Sandwich} model (\subref{fig:sandwich}) sandwiches
		\textcolor[HTML]{332345}{MetaICL} \citep{min_metaicl_2022} (which we
		separately evaluate only in English) between two complementary translation
		API calls. \textcolor[HTML]{38AAAC}{MetaICL-geWECHSELt}
		(\subref{fig:metaicl-gewechselt}) is the result of applying WECHSEL
		\citep{minixhofer_wechsel_2022} to MetaICL.
		\textcolor[HTML]{357aa2}{GPT2-geWECHSELt+MetaICLA} combines
		\textcolor{Dandelion}{MetaICLA}, an adapter trained on the MetaICL dataset
		and objective, with a TGT-language GPT2 base obtained via WECHSEL.
		\textcolor[HTML]{40498e}{GPT2-geWECHSELt+MetaICLVA} does the same, except
		\textcolor{Dandelion}{MetaICLVA} is trained via targeted distillation with
		supervision provided by MetaICL. For more details, refer to section
		\ref{sec:method}.}
\end{figure*}

\subsection{MetaICL}

Due to the complete lack of prompting/instruction templates in non-English languages, we rely on
MetaICL \citep{min_metaicl_2022}, which circumvents the need for prompt/instruction templates at
train-time and test-time. With MetaICL, a pretrained DOT is fine-tuned by concatenating $k$ examples
of input-output pairs (``shots'') from a variety of tasks and feeding this as input to the model.
The final input-output pair is truncated such that only the input is shown, and the model is trained
to predict the output using a negative log-likelihood objective from a number of possible options.
The trained model is then generalises to unseen tasks presented in the same way by utilizing the $k$
shots provided in the context.  We refer to this model as \textit{MetaICL}.

\subsection{Sandwich}

As a baseline, we consider the obvious solution of simply translating input in the target language
to English, feeding the translation to MetaICL, and translating the output back to the target
language. We refer to this model as \textit{Sandwich}. We make use of Google's Cloud Translation AI
API\footnote{\href{https://cloud.google.com/translate}{https://cloud.google.com/translate}}.

\subsection{WECHSEL}

Aside from translation API calls, to adapt a monolingual DOT from a source language to a target
language we employ WECHSEL \citep{minixhofer_wechsel_2022}, which has shown success in adapting the
small variant of GPT2 (117M parameters) to a number of target languages. WECHSEL works by retraining
the tokenizer into the target language and re-initializing the transformer embedding layers such
that the target embeddings are semantically similar to the source embeddings. This is done by
leveraging existing parallel multilingual static word embeddings. As done by
\citet{de_vries_adapting_2021}, after re-initialization, additional causal language modeling (CLM)
is performed in the target language to account for syntactical differences. Applying WECHSEL to
MetaICL, we obtain what we refer to as \textit{MetaICL-geWECHSELt}.

\subsection{Adapters}\label{sec:method:adapters}

Because we are interested in adapting a fine-tuned DOT (MetaICL), we hypothesize that the additional
CLM at the end of WECHSEL can lead to catastrophic forgetting of the fine-tuning. Furthermore, we
hypothesize that the fine-tuning may contain language-specific information, entangled with the task
information relevant to the fine-tuning objective. To address this issue, inspired by MAD-X
\citep{pfeiffer_mad-x_2020} we train a ``task adapter'' on the same ICL objective and data as
MetaICL with a GPT2 base, obtaining an ``ICL-adapter'', which we refer to as \textit{MetaICLA}.
Adapters introduce ``bottleneck'' dense layers at each transformer layer of their base. The adapter
is trained on a particular objective while the base is kept frozen, allowing for parameter-efficient
and modular fine-tuning. These dense layers consist in a down matrix $\mathbf{W}_{down}$, projecting
the hidden states into a lower dimension $d_{bottleneck}$, a non-linearity $f$, which is applied to
this projection and an up matrix $\mathbf{W}_{up}$ that projects back to the original dimension:
\begin{equation} \mathbf{h} \leftarrow \mathbf{W}_{up} f(\mathbf{W}_{down} \mathbf{h}) + \mathbf{r},
\end{equation} where $r$ is a residual connection. Having separated the task-specific information,
we apply WECHSEL to the GPT2 base, obtaining what we refer to as \textit{GPT2-geWECHSELt}. Adding
MetaICLA to GPT2-geWECHSELt, we obtain a model theoretically capable of ICL in the target language,
\textit{GPT2-geWECHSELt+MetaICLA}.

\subsection{\textsc{PHoDiVA}}

To address situations where repeating fine-tuning is not permissible, either because the data is not
released, the process too complicated or the compute simply not available, we propose
\textsc{PHoDiVA}. Here, instead of repeating ICL fine-tuning, we leverage the fine-tuned MetaICL
checkpoint, using it as a teacher in a modified student-teacher offline distillation
\citep{hinton_distilling_2015} setup. More specifically, before WECHSEL adaptation, we add
a ``vessel'' adapter to a (frozen) GPT2 base, and then perform CLM in the source language (English).
Vessel adapters are exactly the same as task adapters, except that they act as a ``vessel'' for
distilled capabilities rather than as additional parameters for fine-tuning. Rather than predicting
the actual next word, the adapter is trained to predict the next word greedily sampled from the
teacher. The idea is to overfit the adapter to the teacher outputs (hence the greedy sampling).
Because the GPT2 base is frozen and theoretically shares the original language modeling capabilities
of the teacher, we hypothesize that this ``targeted distillation'' can disentangle the fine-tuned
capabilities into the vessel adapter. We use the CLM objective because of the constraint to keep the
distillation process as simple as possible, so to make it advantageous over repeating a potentially
complex fine-tuning process. The only constraint of this method is that the adapter base is the same
pretrained base that was fine-tuned into the teacher. When using MetaICL as the teacher, we refer to
the resulting vessel adapter as \textit{MetaICLVA}. Like in section \ref{sec:method:adapters}, after
applying WECHSEL to a GPT2 base, we can then combine the language-adapted base and MetaICLVA to
obtain \textit{GPT2-geWECHSELt+MetaICLVA}, another model theoretically capable of ICL in the target
language.

\section{Experimental Setup}

We use the PyTorch Lightning Python framework \citep{falcon_pytorch_2019} to implement our work.
Because we envision the direct application of this work to be most useful to smaller companies and
start-ups, we limit our compute to a single 40GB NVIDIA A100 GPU and run jobs for a maximum of 24
hours. Our code and checkpoints are available on
GitHub\footnote{\href{https://github.com/thesofakillers/CLAfICLe}{https://github.com/thesofakillers/CLAfICLe}}.

\subsection{Models}

There exist various possible adapter setups, specifying different configurations of the up and down
weight matrices, non-linearity and residual connection, among other settings. For our work, we use
the \verb_pfeiffer_ configuration from AdapterHub \citep{pfeiffer_adapterhub_2020}.

Regarding MetaICL, \citet{min_metaicl_2022} train a number of variants, releasing checkpoints
however only for variants fine tuning the large version of GPT2 (774M parameters). We base the rest
of our models on the same GPT2 version and use the ``high resource to low resource'' direct MetaICL
checkpoint as we consider this to be the most realistic. We make use of the HuggingFace Transformers
\citep{wolf_transformers_2020} implementation of GPT2 throughout.

\subsection{Evaluation}\label{sec:experiment:eval}

For assessing ICL, we reimplement the same evaluation setup as \citet{min_metaicl_2022}, evaluating
a given model on a suite of tasks, prepending the input with $k=16$ training examples sampled
randomly for each task. For our evaluation metrics, like \citet{min_metaicl_2022}, we use F1 for
tasks where the label options change across examples, and accuracy for tasks where the label options
are always the same. Because the evaluation benchmark used by \citet{min_metaicl_2022} is limited to
English, we develop our own multilingual multi-task benchmark spanning 9 different tasks across
3 languages (English, German, and French). Our benchmark design is restricted to tasks that can be
handled by the MetaICL framework, namely multi-class, single-label tasks. To enable complete
comparisons across languages, we also restrict our benchmark to only contain language-parallel
datasets. Correspondingly, we list the ICL benchmark datasets in Table \ref{tab:benchmark}.

\begingroup
\setlength{\tabcolsep}{0.5pt}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[t]
	\centering
	\caption{The datasets constituting our ICL benchmark. Most originate from pre-existing benchmarks,
		namely XGLUE \citep{liang_xglue_2020} and \citep{lin_common_2021}.}
	\label{tab:benchmark}
	\begin{tabular}{@{}llc@{}}
		\toprule
		\multicolumn{1}{l}{Dataset} & \multicolumn{1}{l}{(Origin)}    & Collection \\ \midrule
		HateCheck                   & \citep{rottger_hatecheck_2021}  & -          \\
		XNLI                        & \citep{conneau_xnli_2018}       & XGLUE      \\
		QAM                         & \citep{liang_xglue_2020}        & XGLUE      \\
		QADSM                       & \citep{liang_xglue_2020}        & XGLUE      \\
		PAWS-X                      & \citep{yang_paws-x_2019}        & XGLUE      \\
		MARC                        & \citep{keung_multilingual_2020} & -          \\
		X-CODAH                     & \citep{lin_common_2021}         & XCSR       \\
		X-CSQA                      & \citep{lin_common_2021}         & XCSR       \\
		Wino-X                      & \citep{emelin_wino-x_2021}      & -          \\ \bottomrule
	\end{tabular}
\end{table}
\endgroup

To evaluate the successful application of WECHSEL to GPT2, we use the same process as
\citet{minixhofer_wechsel_2022}, namely measuring perplexity on a held out test set. For all
language modeling, we use the original release of the OSCAR corpus
\citep{ortiz_suarez_monolingual_2020}.

\subsection{Training}

When performing CLM training, due to our limited compute, we heed the advice of
\citet{geiping_cramming_2022} and pack samples into 1024-token sequences (the maximum length
possible) by separating them with EOS tokens, so to minimize the number of padding tokens and
maximize GPU utilisation. With this we are able to fit a batch size of 2 into memory, while actually
presenting the model with more than two examples per batch in most cases\footnote{This technique is
	also suggested by HuggingFace in their CLM tutorial:
	\href{https://huggingface.co/course/chapter7/6}{https://huggingface.co/course/chapter7/6}.}. We
achieve a virtual batch size of 512 by accumulating gradients over 256 steps. We employ single-epoch
training \citep{komatsuzaki_one_2019} on a total of 600M tokens, which we estimate to be the number
of tokens consumed by our model in a single epoch by running a profiling run on a smaller download.
Based on the information in \citet{geiping_cramming_2022} and \citet{minixhofer_wechsel_2022}, we
decide to use Adam \citep{kingma_adam_2015} with a linear warmup for the first half of training to
a peak learning rate of 5e-4, followed by cosine annealing to 0 by the end of training. When
performing targeted distillation for MetaICLVA, we reduce the linear warmup to the first 10\% of
training to help with our voluntary overfitting. As suggested by \citet{izsak_how_2021}, to maximize
training time, we evaluate on only 0.5\% of the data, logging every 50 steps.

For the ICL training necessary for MetaICLA, we modify \citet{min_metaicl_2022}'s implementation so
to work with adapters. In particular, we use their \verb+HR→LR+ training mixture, which consists of
61 tasks sourced from the \textsc{CrossFit} \citep{ye_crossfit_2021} and \textsc{UnifiedQA}
\citep{khashabi_unifiedqa_2020} benchmarks.

\section{Results and Discussion}
\begin{figure}[t]
	\centering
	\includegraphics{gpt2-w_ppl.pdf}
	\caption{Perplexity on the held out set when performing the recommended CLM training after WECHSEL
		language-adaptation of GPT2. A step corresponds to an optimizer update. We evaluate every 50
		steps.}
	\label{fig:gpt2-w_ppl}
\end{figure}

Fig.\@ \ref{fig:gpt2-w_ppl} shows the performance of GPT2 after around 1k steps of training,
evaluated intrinsically in terms of perplexity. For both French and German, we see perplexity
decrease to sub-50 values, with the French model reaching a perplexity of $\approx$ 28. Both models
are clearly underfit, still monotonically decreasing by the end of the training. These observations
are roughly in-line with \citet{minixhofer_wechsel_2022}'s findings for smaller variants of GPT2,
although we train for much less time and hence are left with higher perplexities. While we believe
our preliminary results suggest WECHSEL scales well to larger models in terms of intrinsic
evaluation, future work may wish to investigate whether this holds for longer training times. The
rest of our work considers, among other questions, the robustness of WECHSEL via extrinsic
evaluation on downstream tasks performed by MetaICL.

\begin{figure*}[ht]
	\includegraphics{baselines.pdf}
	\caption{Performance (max is 1) on a particular language dimension of our multi-task benchmark of
		our two baseline models, MetaICL and Sandwich. The dashed line separates whether a given task uses
		accuracy (left) or F1-score (right) as the performance metric.}
	\label{fig:baselines}
\end{figure*}

Fig.\@ \ref{fig:baselines} shows the performance on each dataset of our benchmark for the two
baseline models, MetaICL and Sandwich. As summarized in Table \ref{tab:results-summary}, Sandwich
performs roughly on par with MetaICL on both target languages, respectively with scores of 0.317 and
0.322 in French and German compared to MetaICL's score of 0.327 in English. We note generally low
scores across all tasks. This is particularly perplexing in the case of MetaICL, scoring around
0.1 points less than with the evaluation ensemble used by \citet{min_metaicl_2022}, where the same
checkpoint was reported scoring 0.417 in the worst case (a 25 \% decrease). While similar values
are reached in certain tasks in our benchmark (e.g. most of XGLUE and WINO-X), it is unclear what
the origin of this discrepancy is, whether due to differences in evaluation implementation or
difficulty of the tasks. Given that \citet{min_metaicl_2022} simply report macro-averaged scores,
it is impossible to verify the latter. Nevertheless, our results suggest that Sandwich-like
solutions may be satisfactory for transferring performance from English to other languages given
the surprisingly closeness of the scores. The decision between using Sandwich or ``properly''
adapted models with the same capabilities then becomes an economic one in terms of the cost of API
calls (for the former) versus the cost of inference plus training (for the latter).

\begin{figure*}[ht]
	\includegraphics{results.pdf}
	\caption{Performance gap on our multi-task benchmark between each of the language-adapted models
		and the ``Sandwich'' baseline. Positive values indicate that the adapted models are
		outperforming the baseline, while negative values indicate the reverse. The dashed line
		separates whether a given task uses accuracy (left) or F1-score (right) as the performance
		metric.}
	\label{fig:results}
\end{figure*}

Fig.\@ \ref{fig:results} shows the difference in performance on each dataset of our benchmark
between the proposed models and Sandwich. In general, we observe that the proposed models
underperform across almost all tasks in both French and German, with the trends aligning at
a task-level (e.g. all models underperform on QAM, by roughly the same amount). As reported in Table
\ref{tab:results-summary}, the best of our proposed models is MetaICL-geWECHSELt, which
underperformed Sandwich by roughly 0.02-0.03 points. This undermines the motivation for the other
two models, which were designed to avoid catastrophic forgetting by separating language and ICL
capabilities via adapters. The results suggest that the tradeoff between catastrophic forgetting and
needing to train ICL-adapters leans in favour of the former in this compute regime. In this sense,
we can conclude that WECHSEL does not suffer tremendously due to catastrophic forgetting when
adapting fine-tuned DOTs such as the MetaICL variant of GPT2.

\begin{table}[ht]
	\centering
	\caption{Average performance (max is 1) across the datasets from our multi-task benchmark for the
		models considered in this work. We use ``W'' as a shorthand for ``geWECHSELt''. We report average
		difference in performance for each proposed alternative to Sandwich. Negative values indicate
		underperformance compared to Sandwich.}
	\label{tab:results-summary}
	\begin{tabular}{@{}rccc@{}}
		\toprule
		\multicolumn{1}{c}{} & en    & fr     & de                             \\ \midrule
		MetaICL              & 0.327 & -      & -                              \\
		Sandwich             & -     & 0.317  & 0.322                          \\ \midrule
		\multicolumn{4}{c}{\textit{Difference in Performance w.r.t. Sandwich}} \\
		MetaICL-W            & -     & -0.020 & -0.026                         \\
		GPT2-W+MetaICLA      & -     & -0.041 & -0.042                         \\
		GPT2-W+MetaICLVA     & -     & -0.036 & -0.045                         \\ \bottomrule
	\end{tabular}
\end{table}

Our work is mainly limited by its preliminary nature. Apart for considering more appropriate
(\textit{sc.} larger) compute scales, future work could investigate training ICL-adapters more
thoroughly, for example by performing hyperparameter optimization or incorporating more recent
adapter research such as AdapterDrop \citep{ruckle_adapterdrop_2021}, AdapterFusion
\citep{pfeiffer_adapterfusion_2021} or Hyper-X \citep{ustun_hyper-x_2022}.

Similarly, we note that the despite our best efforts our ICL evaluation benchmark faces some
limitations. Due to the design restrictions mentioned in Section \ref{sec:experiment:eval} and the
lack of non-English datasets we only evaluate on 9 tasks, despite focusing on high-resource
languages like German and French. This is a small number of tasks compared to what is done in
English, where 10s and 100s of tasks can comprise a multi-task benchmark. Furthermore, many of our
datasets are either human or machine translated versions of English, which can lead to noise
\citep{koppel_translationese_2011}. We hope to raise awareness of the complete English dominance of
the NLP data landscape.

We are also interested in a more complete treatment of \textsc{PHoDiVA}. For instance, future work
could explore different forms of student-teacher distillation, consider other forms of loss
criterions, use beam search rather than greedy sampling and/or take inspiration from similar
solutions such as \citet{khrulkov_disentangled_2021}'s work on generative models. We believe work in
this direction could benefit from simplifying the problem setting first, by considering a smaller,
encoder-only transformer fine-tuned on a single downstream task on a single-language.

Other future work may consider different adaptation approaches that have recently emerged. For
example, \citet{marchisio_mini-model_2022}'s Mini-Model adaptation has yet to be tested on
decoder-only transformers, and it would be interesting to see how it compares to WECHSEL in this
regard. Other, slightly more distant approaches such as meta-learning a-la X-MAML
\citep{nooralahzadeh_zero-shot_2020} may provide different results.

Perhaps a clear limitation of this direction of research is that the setting remains monolingual.
Future work could explore whether it is possible to adapt a monolingual model to multiple languages
simultaneously, and how such adaptations would compare to monolingual-to-monolingual adaptation in
terms of resources and performance. Finally, undermining all of this work is our restriction to
results on a single random seed. Future work with more seeds and more compute would be necessary to
draw more definitive conclusions.


\section{Conclusion}

We explore the problem of language-adapting a monolingual DOT previously fine-tuned to perform
in-context learning. To this end, we stress test the current SoTA adaptation method, WECHSEL,
scaling to previously untested model sizes, applying it a fine-tuned variant of GPT2 (MetaICL) and
evaluating extrinsically on a multi-task benchmark. While we find that WECHSEL successfully scales
to larger model sizes, we find that at our compute regime, WECHSEL-adapted MetaICL underperforms
compared to simply sandwiching the English model between translation API calls. We experiment with
separating ICL fine-tuning and language adaptation to address potential catastrophic forgetting
through the use of Adapters, but find these approaches unsuccessful. In doing so, we propose
\textsc{PHoDiVA}, a novel method for post-hoc disentanglement through vessel adapters. We share
\textsc{PHoDiVA} in this rudimentary form as a starting point for future work in this direction.

\section{Acknowledgements}

We would like to thank Sami Jullien and Mozhdeh Ariannezhad for their helpful feedback and general
supervision throughout this project. We would also like to thank Prof. Katia Shutova for acting as
the examiner. Finally we thank the University of Amsterdam for providing the opportunity and
resources necessary for the produced output.

\bibliography{anthology,custom}

\end{document}
